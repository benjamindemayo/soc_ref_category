---
title             : "Children's sensitivity to graded uncertainty"
shorttitle        : "Sensitivity to graded uncertainty"

author: 
  - name          : "Benjamin E. deMayo"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Stanford, CA 94305"
    email         : "bdemayo@stanford.edu"
 

affiliation:
  - id            : "1"
    institution   : " Stanford University"


author_note: |
  


abstract: |
  A growing body of research shows that children between 3 and 5 years old show an emerging ability to monitor their own epistemic uncertainty. However, little is known about children's sensitivity to graded uncertainty and how they adjust their information-seeking behaviors in situations other than those of complete certainty or complete uncertainty. Here, I investigated children's spontaneous social information-gathering behaviors under various levels of graded category uncertainty. Children ages 3-5 (n = 30) were asked to categorize images of creatures which varied in their distance from two prototype images, such that uncertainty about category membership was greater when distance from a prototype was greater. A live eye-tracking paradigm was used to measure children's levels of looking to the experimenter on a trial-by-trial basis. Children looked at the experimenter at the same rate regardless of an item's category ambiguity, suggesting that they did not adjust their information-seeking based on graded category uncertainty.

  
keywords          : "information gathering; graded uncertainty; category discrimination"
wordcount         : "X"

bibliography      : ["soc_ref_category_bib.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---

```{r load_packages, include = FALSE}
library("papaja")
library(psych)
library(langcog)
library(tidyverse)
library(stringr)
library(magrittr)
library(lmerTest)


example_file <- "../raw_data/020218_01-eye_data Samples.txt"

example_trial_file <- "../trial_data/020218_01.csv"

demographics_sheet <- "../soc_ref_cat_demo_github.csv"

key <- "../trial_morphs.csv"

```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
```

#Introduction

Early in life, children are presented with a great deal of social information from which they can learn about the world around them. They overhear conversations, are spoken to directly, and interact with other people nonverbally.  But when do they actively seek out information from their social partners? Specifically, do children "know what they don't know" well enough to seek information on the basis of their own subjective uncertainty, or do they learn more often by passively absorbing information from other people? 

While it may seem intuitive that a child will seek help from a conversational partner when they lack enough information to make a decision, the questions posed above are non-trivial. Prior research which shows that children actively explore their environments and guide their own learning processes [e.g., @schulz2007serious] is somewhat at odds with a separate body of research that highlights young children's deficits when it comes to monitoring their own knowledge states and acting on their own uncertainty [e.g., @markman1977realizing]. In light of this apparent tension, the current study seeks to better understand to what degree children's information-gathering behaviors -- in this case, gaze to a knowledgeable social partner -- are driven by the child's epistemic uncertainty. Building on prior work that suggests that children's social gaze may be a sensitive measure of epistemic uncertainty [@hembacherchildren], this study uses a categorization task to examine whether children's social information-gathering behaviors are modulated by graded uncertainty.

Below, I outline research that highlights differing accounts of children's metacognition which show that children both engage in spontaneous information-seeking behaviors to explore their environments actively and guide their own learning, and that children also have a limited capacity to explicitly report on their own uncertainty. I then review studies demonstrating children's ability to seek information selectively from social partners who could provide helpful information when uncertainty is high.

##Uncertainty Monitoring and Active Learning in Early Childhood

Prior work on children's uncertainty monitoring has yielded mixed evidence as to whether young children can successfully recognize when they lack information and subsequently act on this knowledge of their own ignorance to make a decision [@sodian2012metacognition].  Early studies investigating metacognitive development indicated that children's competence in assessing and acting on their own epistemic ignorance is limited. Markman [-@markman1977realizing] explained a card game to first and third grade children with ambiguous language such that understanding the object of the game was impossible without seeking clarification. An experimenter then prompted the child to seek this clarification with scripted statements such as "What do you think?", "Do you have any questions?" and "Did I tell you everything you need to know to play the game?" In this study, Markman found that the younger children required more prompts before seeking out additional information, which she argues is an indication that young elementary school children lack the capacity to introspect on their own non-understanding. 

Similarly, Flavell, Speer, Green, August, & Whitehurst [-@flavell1981development] asked kindergarten and second-grade children to reconstruct block structures built by a confederate based on a tape of instructions from the confederate, which were not sufficiently clear to successfully recreate the structure.  The researchers found that older children were more likely to indicate the need for additional clarifying information by replaying the tape, asking questions explicitly, or generally looking puzzled, while kindergarteners typically did not display these behaviors. This showed that older children were more adept at seeking information explicitly, but it also demonstrated that younger children were less likely to spontaneously seek information from the environment (for example, by replaying the tape). Below, I discuss later research which shows that young children do engage in spontaneous information-gathering, a finding which is in conflict with this early study, indicating that perhaps recognizing insufficient instructions is more demanding for children than monitoring epistemic uncertainty in other contexts such as incomplete memory. While these studies suggest that young children struggle to articulate their uncertainty through explicit responses, these and other initial investigations of children's uncertainty monitoring required verbal responses from children, a task demand which may systematically underestimate children's metacognitive abilities.

More recent research has examined children's metacognition without requiring children to respond verbally. When asked to report confidence in their answers on a memory task, 3-year-olds were equally confident about correct and incorrect responses, while 4- and 5-year-olds showed a more adult-like pattern of lower confidence for incorrect responses[@hembacher2014don]. In a perceptual identification task, 3- to 5-year-olds reported being less confident when they provided incorrect responses, although overall they were still overconfident [@coughlin2015introspection]. However, the possibility exists that even these studies underestimate children's ability to monitor their own uncertainty because they require children to explicitly report their confidence (even if not in an open-ended verbal response), and that children can act on their own uncertainty before they can communicate it through explicit responses or reflect on it consciously.

In contrast to the explicit confidence measurements used in the studies discussed above, some research has shown that children engage in spontaneous information-gathering behaviors -- that is, behaviors that are not prompted explicitly by an experimenter -- more often when they are faced with uncertainty. These studies demonstrate that children engage in active learning, in which the learner decides what information they want to be exposed to, rather than having the decision made for them by a teacher [@gureckis2012self].  In one study, when 2-year-old children were asked to find a sticker hidden inside of one of several tubes, they peeked inside the tubes more when they had not seen the experimenter place the sticker, indicating that they spontaneously sought out more information when deprived of sufficient information to make a decision [@call2001apes]. In another study, young children were shown a group of toys that either differed in their color or in how they felt to the touch [@robinson2008children]. All of the toys were put into a bag by the experimenter, who mixed them up and then picked out a toy. In the condition where the experimenter put the toy on the table and ask the child, "Which one is it?", children more often reached out and touched the toy more when its distinctive quality was texture than when it was color. In contrast, when the experimenter handed the object to the child, thus giving the child visual and tactile access to the toy simultaneously, children still correctly were able to identify whether the toy was soft or hard, but unable to accurately report that they had arrived at this conclusion by touching the toy. This result demonstrates that children interact with the environment using sensory modalities that will most effectively fill in gaps in uncertainty, even if they are unable to explicitly report doing so.  

Schulz and Bonawitz [-@schulz2007serious] also found that when young children were presented with a causally-confounded toy (i.e.,, a toy that exhibited a behavior whose causal structure could not be deduced) and could choose between playing with that toy or a novel toy, they chose to further explore the causally-confounded toy. However, when the choice was between continuing to play with a causally-unconfounded toy and playing with a new toy, children preferred the novelty of exploring the new toy, demonstrating that children play spontaneously in order to maximize learning based on uncertainty about the world. Goupil & Kouider [-@goupil2016infants] found that even preverbal infants faced with an object-retrieval task persisted on their answers more when they were correct than incorrect, showing that children are sensitive to their own likely accuracy and flexibly adjust their actions accordingly. 

##Children's Social Referencing and Social-Information Gathering

Taken together, these studies provide compelling evidence that children choose to seek more information specifically to resolve uncertainty. In this study, I examine the selectivity of children's spontaneous searches for social information. In order to do this, children must consider other people to be helpful sources of information. 

Evidence in a number of experimental contexts suggests that infants and young children reference trusted adults for disambiguating information in situations of variable outcomes and potential danger.  Sorce, Emde, Campos, and Klinnert [-@sorce1985maternal] showed that infants look to their mothers to determine whether it is safe to proceed crawling over a "visual-cliff", a glass table that is actually safe to walk on but which looks like a daunting drop-off to an infant . When infants' caregivers showed an encouraging expression, infants tended to crawl onto the glass surface, whereas they avoided doing so when their mother showed a frightened facial expression. Other studies have shown that children use caregivers' emotional appraisals of novel toys [@klinnert1981infants; @feinman1982social; @hornik1987effects] and recently-met strangers (Feinman & Lewis, 1983) [@feinman1983social] to inform their own interactions with their environments. 

The above studies provide strong evidence that children, starting in the early stages of infancy, use others' appraisals of the world around them to disambiguate uncertainty about the safety or physical affordances of the environment. However, there is only limited prior research that sheds light on whether children's searches for social information are driven by gaps in the child's knowledge and the child's own awareness of these gaps. While prior research has demonstrated that children selectively seek out social information in certain contexts (i.e., ones in which their physical safety is in question), it is unclear to what extent children will engage in these information-gathering behaviors across other cognitive domains, particularly given the research discussed above which demonstrates that young children struggle to report their own ignorance. If uncertainty about the world did not play a role in children's decision to gather information from the people around then, children would not be selective in when they seek information. Below, I review studies that demonstrate that children are selective when choosing their informants, and that they more heavily weight social information in situations of uncertainty.

##Selective trust in testimony

Some evidence suggests that young children are selective social learners in terms of whom they choose as their informants. Younger children tend to use an informant's familiarity as a cue to the trustworthiness of the information being offered, but children gradually shift to depend more on an informant's demonstrated competence and expertise as they grow older [@corriveau2009choosing; @lucas2013social]. In one study, children were found to generally be proficient at tracking the accuracy of an informant's information when explicitly probed. Furthermore, children who showed this proficiency selectively learned novel object labels from an accurate informant, while those who failed to discriminate between an accurate and inaccurate informant likewise did not selectively use information from the accurate informant [@koenig2004trust].  Children also show selective learning from an informant who is uncertain versus one who is clearly knowledgeable, and are able to assess someone else's knowledge on a particular topic beyond perceptual cues such as facial expression [@sabbagh2001learning]. Taken together, these studies show that children, beginning in the preschool years, choose their information sources in order to maximize knowledge gain. However, it remains unclear whether children are similarly selective in their social information-gathering depending on their own level of epistemic ignorance.

##Selective social learning on the basis of uncertainty

Despite young children's apparent lack of ability to explicitly report their own uncertainty, a few studies indicate that young children may seek and use information selectively from other people on the basis of epistemic uncertainty. Tamis-LeMonda et al. [-@tamis2008infants] extended the finding of Sorce et al. [@sorce1985maternal] in order to determine whether children consider the expressions of their caregivers more when the safety of the environment is more uncertain versus when it is completely certain. In this study, researchers tested whether 18-month-old infants would walk down slopes of varying risk levels when given either encouraging or discouraging signals from their mothers.  The researchers found that children tended to ignore their mothers' encouraging messages when faced with a slope that was clearly too steep to walk down safely, and similarly ignored mothers' discouraging messages when the slope was obviously safe. At borderline slopes, when it was unclear to infants whether the slope was safe to walk down or not, children tended to comply much more consistently with the encouragement or discouragement provided by mother, showing that even  infants selectively give more weight to social information when perceptual input is not enough to make a decision. 

While the study by Tamis-LeMonda et al. examined children's selective integration of social information, it did not include children's solicitations (e.g., through gaze shifts) of social information as a dependent measure. This leaves open the question of whether children actively seek out information from their social partners when uncertainty is high, but some recent work has begun to answer this question. 

Vredenburgh and Kushnir [-@vredenburgh2016young] investigated children's propensity to ask questions when faced with toy-assembly tasks of differential levels of difficulty. They found that preschoolers were significantly more likely to ask an experimenter for help on tasks that were more difficult for them, indicating that children adjust their social information-gathering behavior to optimize their learning. Another study examined whether infants selectively reference their social partners' gaze when their social partners has uttered an ambiguous object label. In this study, infants heard an experimenter produce a novel label when there were either two or one single objects present  [@vaish2011thirteen]. Infants looked up more at the experimenter when there were two objects present and the referent of the experimenter's label was thus ambiguous. 

A later study by Hembacher, deMayo, and Frank (2017) sought to replicate and extend the finding of Vaish et al. with preschool-aged children. In this study, children aged 2 - 5 were presented with either one or two objects, heard a label produced by the experimenter, and asked to put the labeled item in a bucket. The number of objects (1 vs. 2) and their familiarity to the child were varied to manipulate referential ambiguity. Across two experiments, they found that children looked up at the experimenter more as they were making their choice when referential ambiguity was present. Specifically, children did the most looking at the experimenter when both objects were novel, did the least looking when both objects were familiar, and did an intermediate amount when one of the objects was familiar and one was novel. Additionally, when helpful gaze from the experimenter was included as a between-subjects manipulation, children looked up at the experimenter more when one object was familiar and one was novel, but only when helpful gaze was absent, suggesting that children's social referencing might be sensitive to graded levels of epistemic uncertainty. Thus, children are not only sensitive to states of complete ignorance and complete knowledge, but may also be sensitive to intermediate levels of evidence for a hypothesis. 

##Sensitivity to intermediate and graded uncertainty

Although these results provide initial evidence that children seek social information on the basis of graded uncertainty, this study did not manipulate ambiguity in a continuous fashion, but rather presented children with only one intermediate level of uncertainty between complete certainty and complete ambiguity. In many learning contexts (i.e., category learning) many more levels of ambiguity exist than complete certainty and complete ambiguity. 

It is unclear whether young children are sensitive to intermediate levels of epistemic uncertainty, but some research suggests that their capacity to monitor graded uncertainty may be limited. One body of research has shown that children over the age of 3 can report on states of complete ignorance or complete knowledge when asked about an object that is being hidden [e.g., @wimmer1988children], but struggle in partial exposure tasks, in which they are shown several objects but are not shown which one is hidden [@kloo2012development; @sodian1987children].  However, some studies show that children as young as 2 or 3 years old engage in spontaneous information-seeking when faced with ambiguity in partial exposure tasks [@call2001apes; @kloo2017direct]. In sum, while 3 to 6-year-old children struggle to demonstrate explicit or verbal reasoning about their own ignorance in partial exposure tasks, some studies demonstrate that may engage in spontaneous information-gathering when they lack sufficient knowledge to make a decision.

Morgan, Laland and Harris [-@morgan2015development] also investigated young children's sensitivity to graded uncertainty in a different paradigm. In their study, the researchers asked children between the ages of 3 and 7 to determine which of two illustrated quantities was bigger; on some trials, the quantities were more similar to each other (i.e., the task was more difficult) than in other trials. The children then received feedback on their choice from a group of ten informants whose level of consensus was manipulated as an independent variable in the experiment. After receiving feedback the group of ten informants, children could either stick with their initial answer or change it.  Children of all ages in the study seemed to be generally insensitive to their own uncertainty, evidenced by the fact that they tended to stick with their original answers regardless of the initial difficulty of the task. When they did switch their answers, however, children were more likely to switch to the choice of the group when consensus was high, especially among older children. Thus, although children seemed to not adjust their usage of social information when they faced a more ambiguous problem, they were sensitive to the graded strength of social information when they did decide to use it.   This result may have been specific to the task at hand: this study measured children's awareness of graded uncertainty based on whether they were willing to break from their original choice, a metric which may underestimate children's metacognitive reasoning. Furthermore, the studies discussed above show that, even before they are able to report on graded levels of epistemic uncertainty explicitly, preschool-age children spontaneously seek information more often in situations of greater uncertainty. Building off these findings, the current study was uses a categorization paradigm to determine whether children's social information-gathering is driven by graded uncertainty.

##Current Study

Children completed a category discrimination game in which they were introduced to two creatures, and told that each creature had a specific box that it was supposed to live in. They were then handed a number of images depicting creatures at various points on the perceptual continuum between the two prototype images to which they had initially been exposed, and were asked to put each creature in its correct box. The critical dependent measure was the duration of children's looks to the experimenter seated across the table from them, which I measured with an SMI RED-n eye-tracker. I predicted that children would look longer to the experimenter when attempting to categorize stimuli closer to the center of the perceptual continuum. This design allowed me to examine the degree to which children's social referencing is sensitive to graded uncertainty.

A categorization task was used here in part because category learning is a critical learning context in early childhood. Young children possess many mechanisms that facilitate category learning, including sensitivity to statistical regularities in the environment [@saffran1996statistical], sensitivity towards correlations of shared attributes among objects [@younger1985segregation], and acute attention to perceptual features such as shape [@imai1994children]. Selectively seeking social information on the basis of graded uncertainty could be another such mechanism that aids in children's category learning.  It is an open question whether children seek out disambiguating evidence when there is less evidence for an object's category membership.

In this study, I introduced children to two different types of "imaginary creatures" based on stimuli created by Havy and Waxman [-@havy2016naming]. These stimuli sets consisted of two different perceptual continua, such that an image's category identity was more ambiguous as the center of the perceptual continuum is approached. I then asked children to categorize each image. I was interested in whether children's looks to the experimenter would reflect graded levels of category uncertainty, such that children would spend more time looking at the experimenter when the target creature was closer to the center of continuum. I predicted that children's looking would show this pattern.  

Eye-tracking technology was used in this study to allow me to measure the duration of children's looks to the experimenter with greater precision than that afforded by hand-coding. Typically, research in social cognition that uses eye-tracking (particularly when it involves children) measures children's gaze patterns on a digital stimulus display such as a television screen which shows still or moving images  [@gredeback2009eye]. This is partially due to the fact that live-eye tracking with both children and adults is more difficult than conventional eye-tracking displays which use a digital stimulus display. Nevertheless, some research suggests that social looking behaviors may differ slightly when a participant is engaged in a real social interaction as opposed to viewing a screen image of the same interaction [@risko2012social].  One study in particular suggests that merely the potential for social interaction that exists with a live partner draws social gaze more than video images of the same social partner [@laidlaw2011potential]. 

Despite the methodological challenges associated with live eye tracking, its use with children and infants is not unprecedented. This technique has mostly been used to examine social gaze irregularities in children with Autism Spectrum Disorder (ASD), and has been used with children of preschool and primary school age [@noris2012investigating; @falck2015eye; @nadig2010does]. At least one study has also extended this type of eye-tracking to typically-developing infants. Gredeback, Fikke, and Melinder examined the development of infants' gaze-following behaviors between 2 and 8 months and using a live eye-tracking paradigm (-@gredeback2010development). In this study, infants began showing gaze-following behaviors between 2 to 4 months of age, and were more likely to follow the gaze of a stranger than the gaze of their mother. 

In sum, since prior research indicated that such a live eye-tracking approach was possible, and since using an eye tracker decreases the potential for human error involved in hand-coding, I measured gaze shifts to a real-life social partner. 


# Methods

## Participants
I recruited a sample of 30 children aged 3 to 5 at the Bing Nursery School at Stanford University in Stanford, CA. Children were asked by the experimenter if they wanted to play a short game during their regularly scheduled free-play. Children were predominantly white and Asian and generally had highly-educated parents. All children were tested by the same experimenter and were randomly assigned to one of four counterbalance conditions.

## Stimuli, Design and Procedure

### a) Stimuli.
Stimuli were slightly altered versions of those created by Havy and Waxman (2016). Havy and Waxman created their stimulus sets by rendering two novel prototype images and using Norrkross MorphX to create a perceptual continuum of stimulus items that were different morph combinations of the two prototypes. Since pilot testing revealed that the current task was too easy for children when the prototypes varied in color, we transformed them to be monochromatic blue (Figure ##).  The images were printed and laminated to make rectangular cards that children could insert into one of the two boxes.

### b) Experimental Setup and Eye-tracking Mechanism.
Children were seated in a small chair in front of a table which contained two boxes, one red and one green, both placed within their reach. The boxes had openings in the lids to allow children to deposit the stimulus cards into them. When children entered the room, they sat across an X-centimeter-wide table from a large poster-board which displayed the images for the eye-tracking calibration procedure. After the calibration phase had ended and the experimental trials began, the poster-board was replaced with the experimenter sitting in a chair across the table from the child. 

I used an SMI REDn corneal reflection eye-tracker to measure children's gaze shifts to the experimenter throughout the task. The eye-tracking device was placed on two magnetic mounts in the middle of the table, approximately X centimeters in front of the child and set an angle of X degrees to the child's face. Critically, the eye-tracker in this case was measuring the child's gaze shifts to the face of a real person instead of an image on a screen. The child's field of vision was captured by an external-viewing webcam situated directly behind and slightly above the child's head. 

### c) Calibration.
Children were directed to look at each of the 5 calibration images on the poster-boarded situated across the table from them while the experimenter approved each point one-by-one. The calibration display consisted of a large white poster board with five cartoon images (a smiling star, a puppy, a blue fish, a flower, and a red apple) as illustrated in Figure X (in the caption for the figure, explain the geometric configuration of the objects. After the experimenter had approved each calibration point, the child was asked to follow the experimenter's finger as it moved across the calibration display while the experimenter checked to see whether the eye-tracker was accurately representing the gaze location of the child. If the experimenter deemed this informal test of calibration accuracy to be sufficient, the experiment continued; if not, calibration was repeated. (Limitations of this sort of accuracy check are addressed in the Discussion).

### d) Procedure.
Upon entering, children were first asked to write their name on a sticker chart that would be used throughout the task to keep them motivated and prevent them from getting fatigued during the experiment, which lasted about 12 minutes.  After they had written their name, the calibration phase began.

Once the calibration phase was finished, the experimenter removed the poster board and sat across from the child. The experimenter then explained to the child that he had brought pictures of imaginary creatures with him, and that he needed the child's help putting them back in their boxes. The child then categorized six practice items: two items of a 90% morph, two items of a 55% morph, and two items of a 95% morph. After the practice trials, the child completed 2 blocks of 7 test trials, each of which consisted of a 50% morph (completely ambiguous) and two each of 60%, 70%, and 100% morphs (identical to the prototypes). The morphs were presented in one of four pseudo-random orders that were counterbalanced across participants. After the first two test blocks, the child was introduced to a new pair of prototype creatures and asked to complete the same task with these new creatures. They then completed two new test blocks with the new set of creatures, consisting of the same ambiguity levels as the first two test blocks. In total, there were 6 practice and 28 test trials per child. In between each test or practice block, children got to choose a sticker for the chart they had made at the beginning of the experiment and were then reminded of the two prototype creatures at each end of the perceptual continuum. Pilot testing indicated that this  procedure invoked differential levels of accuracy based on stimulus ambiguity and avoided performance degradation by continually re-introducing the prototypes and offering a motivational reward after each block of 7 trials (Figure X). 

### Analysis
Each session was videotaped on a camera behind the experimenter's seat, allowing for view of the entire experimental setup. Accuracy and trial onsets and offsets were hand-coded using DataVyu software (`http://datavyu.org`). A trial was defined as the span of time between when the stimulus item became available to the child (i.e., the experimenter let go of it) and when the item was deposited in a box. All stimuli, data, and analyses are available at `https://github.com/benjamindemayo/soc_ref_category`.

```{r include=FALSE}
# Read in trial data.

demographics <- read_csv(demographics_sheet)
include <- 
  demographics %>% 
  filter(Exclude == "n") 

trial_key <- read_csv(key)

d_trials <- data_frame()
files <- dir("../trial_data")

for (f in files) {
  
  subj_id <- str_replace(f, ".csv", "")
  
  condition <- 
    demographics %>% 
    filter(`SID` == subj_id) %>% 
    pull("Condition")

  jf <- paste("../trial_data/", f, sep = "")
  jd <- read_csv(jf)
  id <- 
    tibble(
      SID = subj_id, 
      trial = jd$Trial.Number,  
      onset = jd$Trial.onset / 1000,  
      offset = jd$Trial.offset / 1000,
      chosen_side = jd$Trial.side
    ) %>%
    mutate(
      camera_onset = jd$camera.onset[1] / 1000,
      webcam_onset = jd$webcam.onset[1] / 1000,
      camera_lag = webcam_onset - camera_onset,
      webcam_length = jd$extcam_length.onset[1] / 1000,
      onset = onset + camera_lag,
      offset = offset + camera_lag

    )
  d_trials <- bind_rows(d_trials, id)
  
}

d_trials <- 
  d_trials  %>%
  left_join(
    demographics %>% select(SID, Condition),
    by = c("SID")
  ) %>%
  left_join(
    trial_key,
    by = c("trial" = "Trial", "Condition")
  ) %>%
  mutate(
    accuracy = case_when(
       correct_side == "M" ~ NA,
       correct_side != "M" ~ correct_side == chosen_side
    )
   ) %>%
  left_join(
    demographics %>% select(SID, age),
    by = "SID"
  )
```


```{r include=FALSE}
#This function reads in data that end in .txt

read.smi.idf <- function(file.name) {
  d <- 
    read_tsv(
    file.name, 
    comment = "##"
    ) %>% 
    filter(Type == "SMP") %>% 
    select(
      t = Time,
      lx = "L POR X [px]",
      rx = "R POR X [px]",
      ly = "L POR Y [px]",
      ry = "R POR Y [px]"
    ) %>% 
    mutate(sid = str_extract(file.name, "\\d{6}_\\d{2}"))
}
```

```{r include=FALSE}
################################################################################
## PREPROCESS DATA 
## take data file with l and r, x and y, as well as stimulus, average
## eyes, do whatever preprocessing needs to be done. 
################################################################################

preprocess.data <- function(d, x.max = 1920, y.max=1080, samp.rate = 30) {

  #Remove out of range looks
    d <- 
    d %>% 
    mutate(
      rx = if_else(rx <= 0 | rx >= x.max, NA_real_, rx),
      lx = if_else(lx <= 0 | lx >= x.max, NA_real_, lx),
      ry = if_else(ry <= 0 | ry >= y.max, NA_real_, ry),
      ly = if_else(ly <= 0 | ly >= y.max, NA_real_, ly)
    )

  #Take one eye if we only have one; otherwise average them
    d <-
      d %>%
      mutate(
        x = case_when(
          is.na(rx) & !is.na(lx) ~ lx,
          !is.na(rx) & is.na(lx) ~ rx,
          !is.na(rx) & !is.na(lx) ~ (rx + lx) / 2,
          is.na(rx) & is.na(lx) ~ NA_real_
        ),
        y = case_when(
          is.na(ry) & !is.na(ly) ~ ly,
          !is.na(ry) & is.na(ly) ~ ry,
          !is.na(ry) & !is.na(ly) ~ (ry + ly) / 2,
          is.na(ry) & is.na(ly) ~ NA_real_
        )
      ) %>%
      select(
        -rx, -ry, -lx, -ly
      ) %>%
      mutate(
        t = round((d$t - d$t[1])/(1000000), 3),
        y = y.max - y
      )
}
```

```{r include=FALSE}
#Read in Experiment data and preprocess.

raw.data.path <- "../raw_data/"
processed.data.path <- "../processed_data/"

## LOOP TO READ IN FILES
all_data <- data_frame()
files <- dir(raw.data.path,pattern = "*.txt")

to.n <- function(x) {as.numeric(as.character(x))}
sem <- function(x) {sd(x) / sqrt(length(x))}

a <-  0
for (file.name in files) {
  a = a + 1
}

for (file.name in files) {
  ## print file name, so if loop breaks, we know where
  print(file.name)
  exp_trials <-
    d_trials %>%
    filter(SID == str_extract(file.name, "\\d{6}_\\d{2}"))
  
  print(exp_trials)

  ## these are the two functions that are most meaningful
  d <- read.smi.idf(str_c(raw.data.path, file.name, sep = ""))
  d <- preprocess.data(d, x.max = 1920, y.max = 1080, samp.rate = 30) 
  
  calibration_time <- d$t[nrow(d)] - exp_trials$webcam_length[1]

  d %<>% 
    mutate(t = t - calibration_time) %>% 
    filter(t >= 0)
  
  d_trial <- 
    exp_trials %>%
    split(.$trial) %>%
    map_df(function(trial_row) {
      this_trial <- filter(d, t >= trial_row$onset, t < trial_row$offset) %>%
        mutate(trial = trial_row$trial) %>%
        left_join(trial_row)
      
      return(this_trial)
    })
  
  ## now here's where data get bound togetherq
  all_data <- bind_rows(all_data, d_trial)
}

write_csv(all_data, path = paste0(processed.data.path, "socref_cat_data.csv"))
```
# Results

## Procedural and Manipulation checks

```{r include=FALSE}
#Accuracy checks.

d_trials_block <- 
  d_trials %>% 
  mutate(
    block = case_when(
      1 <= trial & trial <= 7 ~ 1,
      8 <= trial & trial <= 14 ~ 2,
      15 <= trial & trial <= 21 ~ 3,
      22 <= trial & trial <= 28 ~ 4
    )
  )

d_accuracy_morph <- 
  d_trials_block %>%
  filter(Morph != 50) %>% 
  group_by(SID, Morph) %>% 
  summarize(accuracy = mean(accuracy, na.rm = TRUE)) %>% 
  group_by(Morph) %>% 
  multi_boot_standard(col = "accuracy") %>% 
  ungroup()

d_accuracy_block <- 
  d_trials_block %>% 
  filter(!is.na(accuracy)) %>% 
  group_by(SID, block) %>% 
  summarize(accuracy = mean(accuracy, na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(block) %>% 
  multi_boot_standard(col = "accuracy") %>% 
  ungroup()

d_accuracy_block_morph <- 
  d_trials_block %>% 
  filter(Morph != 50) %>% 
  group_by(SID, Morph, block) %>% 
  summarize(accuracy = mean(accuracy, na.rm = TRUE)) %>% 
  group_by(Morph, block) %>% 
  multi_boot_standard(col = "accuracy") %>% 
  ungroup()
```

```{r fig.cap='Categorization accuracy for each morph level. Error bars are 95 percent confidence intervals.'}

d_accuracy_morph %>% 
  ggplot(aes(x = Morph, y = mean)) + 
  geom_line(size = .5) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  geom_hline(yintercept = .5, lty = 2) + 
  ylim(0,1) + 
  ylab("Proportion Accurate") + 
  xlab("Morph level") + 
  labs(
    title = "Mean Accuracy by Morph Level"
  ) +
  ggthemes::theme_few() +
  scale_x_continuous(breaks = c(60, 70, 100)) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 18),
    axis.title = element_text(size = 16),
    axis.text = element_text(size = 12)
  ) 
```
 
```{r fig.cap="Categorization accuracy for each of four experimental blocks, suggesting that children's accuracy did not degrade throughout the task. Error bars are 95 percent confidence intervals."}

d_accuracy_block %>% 
  ggplot(aes(block, mean)) +
  geom_col() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +
  labs(
    x = "Block",
    y = "Mean Accuracy",
    title = "Mean Accuracy by Test Block"
  ) +
  ggthemes::theme_few() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 18),
    axis.title = element_text(size = 16),
    axis.text = element_text(size = 12)
  ) 
```


To validate the manipulation of category ambiguity in my procedure, I first examined whether children's accuracy on the task corresponded to the category ambiguity of each stimulus item (Figure ##). Children were most accurate when categorizing images of morph level 100, which were identical to the prototype images shown as category exemplars at the beginning of each test block (mean accuracy = `r  d_accuracy_morph %>% filter(Morph == 100) %>% pull(mean)`.) They were less accurate when categorizing images of morph level 70 (mean accuracy = `r d_accuracy_morph %>% filter(Morph == 70) %>% pull(mean)`) and still less accurate when categorizing images of morph level 60 (mean accuracy = `r d_accuracy_morph %>% filter(Morph == 60) %>% pull(mean)`). Thus, as was designed, the task was more difficult for children when they were tasked with categorizing more ambiguous stimuli. Children's performance on the task also seemed to remain consistent throughout the approximately 6 minute duration of test trials (Figure XX), indicating that children remained engaged and motivated throughout the experiment.

```{r include=FALSE}

trial_viz <- 
  all_data %>% 
  group_by(sid, trial) %>% 
  mutate(
    trial_time = t - min(t),
    bin = (trial_time %/% 0.5) * 0.5
  ) %>% 
  ungroup() %>% 
  arrange(bin, sid) %>% 
  mutate(looking = !is.na(x) & !is.na(y)) %>% 
  select(trial_time, bin, looking, everything()) 

trial_viz_n <- 
  trial_viz %>% 
  group_by(bin, sid, trial) %>% 
  slice(1) %>% 
  ungroup() %>% 
  count(bin)

trial_viz_ci <- 
  trial_viz %>% 
  group_by(bin) %>% 
  multi_boot_standard(col = "looking") %>% 
  ungroup() %>% 
  mutate(axis_position = bin + 0.25) %>% 
  left_join(
    trial_viz_n,
    by = c("bin")
  )
```

```{r fig.cap='Timecourse of proportion looking at experimenter, in half-second intervals. Size of points indicates amount of trials represented in each point. Error bars are 95 percent confidence intervals.'}
trial_viz_ci %>% 
  filter(bin <= 10) %>% 
  ggplot(aes(axis_position, mean)) +
  geom_point(aes(size = n)) +
  geom_line() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +
  labs(
    x = "time after beginning of trial (seconds)",
    y = "mean proportion looking at experimenter",
    title = "Time course of proportion looking\nat experimenter across trials",
    size = "Number of trials\nrepresented"
  ) +
  scale_x_continuous(breaks = seq(from = 0, to = 10, by = 0.5)) +
  scale_size_continuous(breaks = seq(from = 700, to = 0, by = -100), range = c(1, 4.5)) +
  ggthemes::theme_few() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 8)
  ) 
```


Since this analysis method required the integration of data acquired from various devices, I examined the average timecourse of children's looking patterns during trials to ensure that data output from these devices was aligning appropriate. Figure ## shows, for each half-second interval after the beginning of the trial, the average proportion of that interval that children spent looking at the experimenter. Since the eye-tracking data collection method in this paradigm was coarse, any looks captured by the eye-tracker were considered to be a look in the general direction of the experimenter; thus, the "Area of Interest" in this design was defined as the entire spatial area for which the eye-tracker was calibrated. 

```{r include=FALSE}

#Trial timecourse visualization, backwards from decision
trial_viz_backwards <-
  all_data %>% 
  group_by(sid, trial) %>% 
  mutate(
    t_before_decision = t - max(t),
    time_bin = (t_before_decision %/% 0.5) * 0.5  
  ) %>% 
  ungroup() %>% 
  arrange(time_bin, sid) %>% 
  mutate(looking = !is.na(x) & !is.na(y)) %>% 
  select(t_before_decision, time_bin, sid, everything())

trial_viz_n_backwards <- 
  trial_viz_backwards %>% 
  group_by(time_bin, sid, trial) %>% 
  slice(1) %>% 
  ungroup() %>% 
  count(time_bin)

trial_viz_ci_backwards <- 
  trial_viz_backwards %>% 
  group_by(time_bin) %>% 
  multi_boot_standard(col = "looking") %>% 
  ungroup() %>% 
  mutate(axis_position = time_bin - 0.25) %>% 
  left_join(
    trial_viz_n_backwards,
    by = c("time_bin")
  )

median_trial_length <- 
  d_trials %>% 
  mutate(trial_length = offset - onset) %>% 
  filter(trial_length > 0) %>% 
  summarise(median = median(trial_length))
```

```{r fig.cap="Timecourse of children's looking at experimenter prior to decision, in half-second intervals. Size of points represents amount of trials included in each data point. Error bars are 95 percent confidence intervals."}

trial_viz_ci_backwards %>% 
  filter(time_bin >= -6) %>% 
  ggplot(aes(axis_position, mean)) +
  geom_point(aes(size = n)) +
  geom_line() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +
  labs(
    x = "time before decision (seconds)",
    y = "mean proportion looking at experimenter",
    title = "Timecourse of children's referencing\nof experimenter prior to decision",
    size = "Number of trials\nrepresented"
  ) +
  scale_size_continuous(
    breaks = seq(from = 700, to = 0, by = -100), 
    range = c(1, 4.5)
  ) +
  scale_x_continuous(breaks = seq(from = -6, to = 0, by = 0.5)) +
  ggthemes::theme_few() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 8)
  ) 



```


In general, Figure ## shows that children tend to look slightly more at the experimenter at the very beginning of the trial, perhaps since this is when they are receiving the stimulus item from the experimenter. As trial length increases, so does the proportion of time in each half-second interval that children spend looking at the experimenter, which may be in part because longer trials frequently featured children asking questions and thus making eye-contact with the experimenter. Figure ## shows the timecourse of children's looking in half-second intervals before the end of the trial, which corresponds to the child's decision to place the stimulus item in one box or another. This plot shows a peak in children's looking behavior approximately 2.5 - 3 seconds before a decision, which is slightly after the beginning of the trial (median trial length = `r median_trial_length %>% pull(1)` seconds). Taken together, these plots show a plausible timecourse of children's looking throughout trials which lends confirmatory evidence that data from various devices was integrated and aligned correctly.

##Proportion of trial looking by morph
Did children selectively seek out more social information when trying to categorize more ambiguous stimulus items? I fit the data to a linear mixed-effects model to quantify the effect of category ambiguity on proportion of trial spent looking at the experimenter, along with any possible developmental trends. I ran a linear mixed-effects model with the following preregistered structure: `proportion of trial spent looking ~ morph * centered age + (morph | participant) + (1 | trial)`. Random effects are denoted in parentheses.

```{r include=FALSE}
trial_proportions <- 
  all_data %>% 
  filter(!is.na(trial)) %>% 
  group_by(sid, trial) %>% 
  mutate(
    trial_length = max(t) - min(t),
    look_captured = !is.na(x) | !is.na(y)
  ) %>% 
  mutate(look = case_when(
    look_captured ~ "looking",
    !look_captured ~ "not_looking"
  )) %>% 
  ungroup() %>% 
  count(sid, trial, look, Morph) %>% 
  spread(look, n) %>% 
  left_join(
    demographics %>% select(SID, age),
    by = c("sid" = "SID")
  ) %>%
  
  replace_na(list(looking = 0, not_looking = 0)) %>% 
  mutate(prop_looking = looking / (looking + not_looking))

trial_lengths <-  
  d_trials %>% 
  mutate(length = offset - onset) %>% 
  filter(length > 0) %>% 
  arrange(length)

morph_proportions <- 
  trial_proportions %>% 
  group_by(Morph) %>% 
  multi_boot_standard(col = "prop_looking")
```

```{r}
lmer_data <- 
  trial_proportions %>% 
  rename(morph = Morph) %>% 
  mutate(
    morph = factor(morph),
    trial = factor(trial),
    age_c = as.numeric(langcog::scale(age, scale = FALSE))
  )

lmer_data$morph <- relevel(lmer_data$morph, ref = "100")
  
maximal_mod <- 
  lmerTest::lmer(
    prop_looking ~ morph * age_c + 
      (morph | sid) + (1 | trial),
    data = lmer_data, REML = FALSE
  )
  
g_lm <- summary(maximal_mod)

morph_model_tab <- as.data.frame(g_lm$coef)
```




# Discussion


\newpage

# References
```{r}
r_refs(file = "soc_ref_category_bib.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

